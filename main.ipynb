{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-24T09:38:36.429458300Z",
     "start_time": "2023-08-24T09:38:36.358696900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "\n",
    "import web_scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###################\n",
    "# opening csv file and put it's content into a list of urls\n",
    "###################\n",
    "csv_contents = []\n",
    "\n",
    "with open(\"resources/furniture stores pages.csv\", \"r\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        csv_contents.append(row[0])\n",
    "\n",
    "csv_contents.remove(\"max(page)\")\n",
    "\n",
    "print(len(csv_contents))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###################\n",
    "# Fetch HTML content for each URL\n",
    "###################\n",
    "\n",
    "html_contents = []\n",
    "\n",
    "for url in csv_contents:\n",
    "    html_content = web_scrapping.fetch_html_content(url)\n",
    "    if html_content:\n",
    "        html_contents.append(html_content)\n",
    "\n",
    "print(f\"Fetched HTML content from {len(html_contents)} URLs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###################\n",
    "# Save fatched HTML content to file for faster following processing\n",
    "###################\n",
    "\n",
    "with open(\"resources/html_content.csv\", \"w\", newline=\"\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(html_contents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "###################\n",
    "# setting new limit for csv reading because the html content si bigger that the actual size can read\n",
    "###################\n",
    "\n",
    "max_int = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int/10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T08:31:11.144149200Z",
     "start_time": "2023-08-23T08:31:11.134060700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "###################\n",
    "# load fatched HTML content to file for faster following processing\n",
    "###################\n",
    "\n",
    "with open(\"resources/html_content.csv\", \"r\") as f:\n",
    "    read = csv.reader(f)\n",
    "\n",
    "    html_contents = list(read)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T08:31:12.450848900Z",
     "start_time": "2023-08-23T08:31:11.952773200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(html_contents[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###################\n",
    "# preprocessing HTML content by removing unecessary tags\n",
    "###################\n",
    "import preprocessing_html\n",
    "\n",
    "for i in range(len(html_contents)):\n",
    "    html_contents[i] = preprocessing_html.preprocess_html(html_contents[i])\n",
    "    print(f\"Preprocessed HTML content for analysis {i}/{len(html_contents)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(html_contents[0])\n",
    "print(max(html_contents[0], key=len))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T09:38:18.590388600Z",
     "start_time": "2023-08-24T09:38:18.559867800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.0.1'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T09:38:31.601951Z",
     "start_time": "2023-08-24T09:38:31.577337500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n",
      "Spark NLP version:  5.0.2\n",
      "FindSpark version:  2.0.1\n",
      "spark version:  3.4.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "spark = SparkSession.builder.appName(\"HelloWorld\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "print(nums.map(lambda x: x*x).collect())\n",
    "\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"FindSpark version: \", findspark.__version__)\n",
    "print(\"spark version: \", spark.version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:49.238961500Z",
     "start_time": "2023-08-24T10:07:41.662848300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Factory Buys 32cm...|\n",
      "|Beadlight Cirrus ...|\n",
      "|Hamar Plant Stand...|\n",
      "|Arrange Console T...|\n",
      "|Artemis Oval Mirr...|\n",
      "|Trim Sideboard 18...|\n",
      "|Aster Door Mat - ...|\n",
      "|Hamar Plant Stand...|\n",
      "|Linear Wood table...|\n",
      "|  Aqua Table PRODUCT|\n",
      "|Taylor Dining Tab...|\n",
      "|A- Joint Tabla PR...|\n",
      "|A- Joint Round Ta...|\n",
      "|Helborn Table PRO...|\n",
      "|Wrongwoods Table ...|\n",
      "|Cork Dining Table...|\n",
      "|Crystal Brook 11 ...|\n",
      "|Hampton's 11 Piec...|\n",
      "|Eastport 11 Piece...|\n",
      "|Republic 13 Piece...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.training import CoNLL\n",
    "training_data = spark.read.text(\"resources/training_data.txt\")\n",
    "training_data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:53.312069300Z",
     "start_time": "2023-08-24T10:07:53.253777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# word_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\")\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# from sparknlp.annotator import BertEmbeddings\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m bert \u001B[38;5;241m=\u001B[39m \u001B[43mWordEmbeddingsModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresources/pretrained_models/glove.6B.100d.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \\\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \\\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39msetCaseSensitive(\u001B[38;5;28;01mFalse\u001B[39;00m) \\\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39msetPoolingLayer(\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# default 0\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\ml\\util.py:353\u001B[0m, in \u001B[0;36mMLReadable.load\u001B[1;34m(cls, path)\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RL:\n\u001B[0;32m    352\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mload(path)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\internal\\annotator_java_ml.py:23\u001B[0m, in \u001B[0;36mAnnotatorJavaMLReadable.read\u001B[1;34m(cls)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread\u001B[39m(\u001B[38;5;28mcls\u001B[39m):\n\u001B[0;32m     22\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns an MLReader instance for this class.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m AnnotatorJavaMLReader(\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\__init__.py:139\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\annotator\\embeddings\\word_embeddings.py:281\u001B[0m, in \u001B[0;36mWordEmbeddingsModel.__init__\u001B[1;34m(self, classname, java_model)\u001B[0m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;129m@keyword_only\u001B[39m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, classname\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcom.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel\u001B[39m\u001B[38;5;124m\"\u001B[39m, java_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 281\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mWordEmbeddingsModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclassname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclassname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjava_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjava_model\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\__init__.py:139\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\common\\annotator_model.py:37\u001B[0m, in \u001B[0;36mAnnotatorModel.__init__\u001B[1;34m(self, classname, java_model)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m classname \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m java_model:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m_java_class_name \u001B[38;5;241m=\u001B[39m classname\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_java_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m java_model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_from_java()\n",
      "File \u001B[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:86\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[1;34m(java_class, *args)\u001B[0m\n\u001B[0;32m     84\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n\u001B[0;32m     85\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjava_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mjava_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "word_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\")\n",
    "# from sparknlp.annotator import BertEmbeddings\n",
    "bert = WordEmbeddingsModel.load(\"resources/pretrained_models/glove.6B.100d.txt\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"bert\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setPoolingLayer(0)  # default 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:09:34.517179300Z",
     "start_time": "2023-08-24T10:09:34.473605700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_dl download started this may take some time.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# load NER model trained by deep learning approach and GloVe word embeddings\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m ner_dl \u001B[38;5;241m=\u001B[39m \u001B[43mNerDLModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mner_dl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# load NER model trained by deep learning approach and BERT word embeddings\u001B[39;00m\n\u001B[0;32m      4\u001B[0m ner_bert \u001B[38;5;241m=\u001B[39m NerDLModel\u001B[38;5;241m.\u001B[39mpretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mner_dl_bert\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\annotator\\ner\\ner_dl.py:586\u001B[0m, in \u001B[0;36mNerDLModel.pretrained\u001B[1;34m(name, lang, remote_loc)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Downloads and loads a pretrained model.\u001B[39;00m\n\u001B[0;32m    569\u001B[0m \n\u001B[0;32m    570\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    583\u001B[0m \u001B[38;5;124;03m    The restored model\u001B[39;00m\n\u001B[0;32m    584\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msparknlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpretrained\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ResourceDownloader\n\u001B[1;32m--> 586\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mResourceDownloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownloadModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNerDLModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremote_loc\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\pretrained\\resource_downloader.py:87\u001B[0m, in \u001B[0;36mResourceDownloader.downloadModel\u001B[1;34m(reader, name, language, remote_loc, j_dwn)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Downloads and loads a model with the default downloader. Usually this method\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03mdoes not need to be called directly, as it is called by the `pretrained()`\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;124;03mmethod of the annotator.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;124;03m    Loaded pretrained annotator/pipeline\u001B[39;00m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mprint\u001B[39m(name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m download started this may take some time.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 87\u001B[0m file_size \u001B[38;5;241m=\u001B[39m \u001B[43m_internal\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_GetResourceSize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremote_loc\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mapply()\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not find the model to download please check the name!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\internal\\__init__.py:384\u001B[0m, in \u001B[0;36m_GetResourceSize.__init__\u001B[1;34m(self, name, language, remote_loc)\u001B[0m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, language, remote_loc):\n\u001B[1;32m--> 384\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m_GetResourceSize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremote_loc\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\internal\\extended_java_wrapper.py:27\u001B[0m, in \u001B[0;36mExtendedJavaWrapper.__init__\u001B[1;34m(self, java_obj, *args)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28msuper\u001B[39m(ExtendedJavaWrapper, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(java_obj)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_java_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjava_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sparknlp\\internal\\extended_java_wrapper.py:37\u001B[0m, in \u001B[0;36mExtendedJavaWrapper.new_java_obj\u001B[1;34m(self, java_class, *args)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnew_java_obj\u001B[39m(\u001B[38;5;28mself\u001B[39m, java_class, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_java_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjava_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:86\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[1;34m(java_class, *args)\u001B[0m\n\u001B[0;32m     84\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n\u001B[0;32m     85\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjava_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mjava_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "# load NER model trained by deep learning approach and GloVe word embeddings\n",
    "ner_dl = NerDLModel.pretrained(\"ner_dl\")\n",
    "# load NER model trained by deep learning approach and BERT word embeddings\n",
    "ner_bert = NerDLModel.pretrained(\"ner_dl_bert\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:07:59.181098700Z",
     "start_time": "2023-08-24T10:07:59.137940600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T09:45:19.477038900Z",
     "start_time": "2023-08-24T09:45:19.459529500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o127.fit.\n: java.lang.Exception: Could not find a column of type word_embeddings. Make sure your pipeline is correct.\r\n\tat com.johnsnowlabs.storage.HasStorageRef$.$anonfun$getStorageRefFromInput$2(HasStorageRef.scala:70)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat com.johnsnowlabs.storage.HasStorageRef$.getStorageRefFromInput(HasStorageRef.scala:69)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach.train(NerDLApproach.scala:490)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach.train(NerDLApproach.scala:180)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:69)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:75)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 10\u001B[0m\n\u001B[0;32m      1\u001B[0m nerTagger \u001B[38;5;241m=\u001B[39m NerDLApproach() \\\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword_embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \\\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39msetLabelColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39msetPo(\u001B[38;5;241m0.005\u001B[39m) \\\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39msetBatchSize(\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m ner_model \u001B[38;5;241m=\u001B[39m \u001B[43mnerTagger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m ner_model\u001B[38;5;241m.\u001B[39mwrite()\u001B[38;5;241m.\u001B[39moverwrite()\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresources/ner_model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m ner_model \u001B[38;5;241m=\u001B[39m NerDLModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath/to/ner_model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[0;32m    210\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[1;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[1;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    171\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o127.fit.\n: java.lang.Exception: Could not find a column of type word_embeddings. Make sure your pipeline is correct.\r\n\tat com.johnsnowlabs.storage.HasStorageRef$.$anonfun$getStorageRefFromInput$2(HasStorageRef.scala:70)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat com.johnsnowlabs.storage.HasStorageRef$.getStorageRefFromInput(HasStorageRef.scala:69)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach.train(NerDLApproach.scala:490)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach.train(NerDLApproach.scala:180)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:69)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:75)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nerTagger = NerDLApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"word_embeddings\"]) \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setOutputCol(\"ner\") \\\n",
    "    .setMaxEpochs(10) \\\n",
    "    .setLr(0.001) \\\n",
    "    .setPo(0.005) \\\n",
    "    .setBatchSize(64)\n",
    "\n",
    "ner_model = nerTagger.fit(training_data)\n",
    "\n",
    "ner_model.write().overwrite().save('resources/ner_model')\n",
    "\n",
    "ner_model = NerDLModel.load('path/to/ner_model')\n",
    "text = \"The new iPhone was released today.\"\n",
    "result = ner_model.transform(spark.createDataFrame([(text,)]).toDF(\"text\"))\n",
    "entities = result.select(\"token.result\", \"ner.result\").collect()\n",
    "\n",
    "for entity in entities[0][1]:\n",
    "    print(entity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:27:36.520980Z",
     "start_time": "2023-08-23T09:27:36.455287800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
